{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 141 artifact(s)\n",
      "Adding 3 artifact(s)\n",
      "Adding 13 artifact(s)\n",
      "Adding 4 artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\"org.apache.spark\" % \"spark-sql_2.11\" % \"1.6.1\")\n",
    "classpath.add(\"com.databricks\" % \"spark-csv_2.11\" % \"1.4.0\")\n",
    "classpath.add(\"org.apache.hadoop\" % \"hadoop-common\" % \"2.7.2\")\n",
    "classpath.add(\"com.github.wookietreiber\" % \"scala-chart_2.11\" % \"0.5.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "16/05/18 22:41:54 INFO SparkContext: Running Spark version 1.6.1\n",
      "16/05/18 22:41:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/18 22:41:54 INFO SecurityManager: Changing view acls to: qhuang\n",
      "16/05/18 22:41:54 INFO SecurityManager: Changing modify acls to: qhuang\n",
      "16/05/18 22:41:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(qhuang); users with modify permissions: Set(qhuang)\n",
      "16/05/18 22:41:56 INFO Utils: Successfully started service 'sparkDriver' on port 62769.\n",
      "16/05/18 22:41:57 INFO Slf4jLogger: Slf4jLogger started\n",
      "16/05/18 22:41:57 INFO Remoting: Starting remoting\n",
      "16/05/18 22:41:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.1.20:62782]\n",
      "16/05/18 22:41:57 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 62782.\n",
      "16/05/18 22:41:57 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/05/18 22:41:57 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/05/18 22:41:57 INFO DiskBlockManager: Created local directory at C:\\Users\\qhuang\\AppData\\Local\\Temp\\blockmgr-39ee11fc-9c01-4d53-a5b2-a1d469592bf3\n",
      "16/05/18 22:41:57 INFO MemoryStore: MemoryStore started with capacity 2.4 GB\n",
      "16/05/18 22:41:58 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/05/18 22:41:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "16/05/18 22:41:58 INFO SparkUI: Started SparkUI at http://192.168.1.20:4040\n",
      "16/05/18 22:41:58 INFO Executor: Starting executor ID driver on host localhost\n",
      "16/05/18 22:41:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62819.\n",
      "16/05/18 22:41:58 INFO NettyBlockTransferService: Server created on 62819\n",
      "16/05/18 22:41:58 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "16/05/18 22:41:58 INFO BlockManagerMasterEndpoint: Registering block manager localhost:62819 with 2.4 GB RAM, BlockManagerId(driver, localhost, 62819)\n",
      "16/05/18 22:41:58 INFO BlockManagerMaster: Registered BlockManager\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkContext\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkConf\u001b[0m\n",
       "\u001b[36msparkConf\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mSparkConf\u001b[0m = org.apache.spark.SparkConf@4afab775\n",
       "\u001b[36msc\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mSparkContext\u001b[0m = org.apache.spark.SparkContext@2337fba4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "@transient val sparkConf = new SparkConf().setAppName(\"Ex3\").setMaster(\"local[*]\")\n",
    "@transient val sc = new SparkContext(sparkConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SQLContext\u001b[0m\n",
       "\u001b[36msqlContext\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32msql\u001b[0m.\u001b[32mSQLContext\u001b[0m = org.apache.spark.sql.SQLContext@3dac0083"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "\n",
    "@transient val sqlContext = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1.0 (TID 3, localhost): java.lang.ArrayIndexOutOfBoundsException: 18002\r",
      "\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.recvDecodingTables(CBZip2InputStream.java:730)\r",
      "\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.getAndMoveToFrontDecode(CBZip2InputStream.java:801)\r",
      "\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.initBlock(CBZip2InputStream.java:504)\r",
      "\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.changeStateToProcessABlock(CBZip2InputStream.java:333)\r",
      "\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.read(CBZip2InputStream.java:399)\r",
      "\tat org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream.read(BZip2Codec.java:483)\r",
      "\tat java.io.InputStream.read(Unknown Source)\r",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)\r",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\r",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)\r",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)\r",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:246)\r",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:208)\r",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\r",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\r",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369)\r",
      "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:413)\r",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:396)\r",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:742)\r",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1194)\r",
      "\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:155)\r",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1194)\r",
      "\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:212)\r",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1194)\r",
      "\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1112)\r",
      "\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1112)\r",
      "\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:1951)\r",
      "\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:1951)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r",
      "\tat java.lang.Thread.run(Unknown Source)\r",
      "",
      "Driver stacktrace: (Job aborted due to stage failure: Task 2 in stage 1.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1.0 (TID 3, localhost): java.lang.ArrayIndexOutOfBoundsException: 18002\r",
      "\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.recvDecodingTables(CBZip2InputStream.java:730)\r",
      "\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.getAndMoveToFrontDecode(CBZip2InputStream.java:801)\r",
      "\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.initBlock(CBZip2InputStream.java:504)\r",
      "\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.changeStateToProcessABlock(CBZip2InputStream.java:333)\r",
      "\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.read(CBZip2InputStream.java:399)\r",
      "\tat org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream.read(BZip2Codec.java:483)\r",
      "\tat java.io.InputStream.read(Unknown Source)\r",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)\r",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\r",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)\r",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)\r",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:246)\r",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:208)\r",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\r",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\r",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369)\r",
      "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:413)\r",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:396)\r",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:742)\r",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1194)\r",
      "\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:155)\r",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1194)\r",
      "\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:212)\r",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1194)\r",
      "\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1112)\r",
      "\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1112)\r",
      "\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:1951)\r",
      "\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:1951)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r",
      "\tat java.lang.Thread.run(Unknown Source)\r",
      "",
      "Driver stacktrace:)",
      "  org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)",
      "  scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)",
      "  scala.Option.foreach(Option.scala:257)",
      "  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)",
      "  org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",
      "  org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)",
      "  org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1114)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)",
      "  org.apache.spark.rdd.RDD.withScope(RDD.scala:316)",
      "  org.apache.spark.rdd.RDD.aggregate(RDD.scala:1107)",
      "  com.databricks.spark.csv.util.InferSchema$.apply(InferSchema.scala:41)",
      "  com.databricks.spark.csv.CsvRelation.inferSchema(CsvRelation.scala:249)",
      "  com.databricks.spark.csv.CsvRelation.<init>(CsvRelation.scala:73)",
      "  com.databricks.spark.csv.DefaultSource.createRelation(DefaultSource.scala:162)",
      "  com.databricks.spark.csv.DefaultSource.createRelation(DefaultSource.scala:44)",
      "  org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:158)",
      "  org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:119)",
      "  org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:109)",
      "  cmd3$$user$$anonfun$6.apply(Main.scala:31)",
      "  cmd3$$user$$anonfun$6.apply(Main.scala:24)",
      "java.lang.ArrayIndexOutOfBoundsException: 18002 (18002)",
      "  org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.recvDecodingTables(CBZip2InputStream.java:730)",
      "  org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.getAndMoveToFrontDecode(CBZip2InputStream.java:801)",
      "  org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.initBlock(CBZip2InputStream.java:504)",
      "  org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.changeStateToProcessABlock(CBZip2InputStream.java:333)",
      "  org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.read(CBZip2InputStream.java:399)",
      "  org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream.read(BZip2Codec.java:483)",
      "  java.io.InputStream.read(Unknown Source)",
      "  org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)",
      "  org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)",
      "  org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)",
      "  org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)",
      "  org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:246)",
      "  org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:208)",
      "  org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)",
      "  org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)",
      "  scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369)",
      "  scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:413)",
      "  scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:396)",
      "  scala.collection.Iterator$class.foreach(Iterator.scala:742)",
      "  scala.collection.AbstractIterator.foreach(Iterator.scala:1194)",
      "  scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:155)",
      "  scala.collection.AbstractIterator.foldLeft(Iterator.scala:1194)",
      "  scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:212)",
      "  scala.collection.AbstractIterator.aggregate(Iterator.scala:1194)",
      "  org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1112)",
      "  org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1112)",
      "  org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:1951)",
      "  org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:1951)",
      "  org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)",
      "  org.apache.spark.scheduler.Task.run(Task.scala:89)",
      "  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)",
      "  java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)",
      "  java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)",
      "  java.lang.Thread.run(Unknown Source)"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.jfree.data.category.DefaultCategoryDataset\n",
    "import scalax.chart.module.ChartFactories\n",
    "\n",
    "val searches = sqlContext.read\n",
    "   .format(\"com.databricks.spark.csv\")\n",
    "   .option(\"header\", \"true\")\n",
    "   .option(\"delimiter\", \"^\")\n",
    "   .option(\"inferSchema\", \"true\")\n",
    "   .option(\"mode\", \"DROPMALFORMED\")\n",
    "   .load(\"dataset/searches.csv.bz2\")\n",
    "searches.registerTempTable(\"searchesTable\")\n",
    "val df = sqlContext.sql(\"select Date, Destination from searchesTable\")\n",
    "val resultAGP = monthlyCount(df, \"AGP\").collect\n",
    "resultAGP.foreach(println)\n",
    "\n",
    "val resultMAD = monthlyCount(df, \"MAD\").collect\n",
    "resultMAD.foreach(println)\n",
    "\n",
    "val resultBCN = monthlyCount(df, \"BCN\").collect\n",
    "resultBCN.foreach(println)\n",
    "createChart(resultAGP, resultMAD, resultBCN)\n",
    "\n",
    "\n",
    "def monthlyCount(df: DataFrame, airport: String): RDD[(String, Long)] = {\n",
    "    df.filter(col(\"Destination\") === airport).map(r => (r.getString(0).substring(0, 7), 1L)).reduceByKey(_+_)\n",
    "}\n",
    "\n",
    "def createChart(resultAGP: Array[(String, Long)], resultMAD: Array[(String, Long)], resultBCN: Array[(String, Long)]) {\n",
    "    val ds = new DefaultCategoryDataset\n",
    "    resultAGP.foreach{ r =>\n",
    "      ds.addValue(r._2, \"AGP\", r._1)\n",
    "    }\n",
    "    resultMAD.foreach{ r =>\n",
    "      ds.addValue(r._2, \"MAD\", r._1)\n",
    "    }\n",
    "    resultBCN.foreach{ r =>\n",
    "      ds.addValue(r._2, \"BCN\", r._1)\n",
    "    }\n",
    "\n",
    "    val chart = ChartFactories.BarChart(ds)\n",
    "    chart.show()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
